{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6fae01",
   "metadata": {},
   "source": [
    "# üî• GPU-Accelerated Training Setup\n",
    "\n",
    "**GPU Detected**: RTX 3070 Ti with 8GB VRAM\n",
    "**CUDA Version**: 12.6\n",
    "**PyTorch**: 2.7.1+cu126\n",
    "\n",
    "This notebook will help you verify your GPU setup and configure optimal training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f01715",
   "metadata": {},
   "source": [
    "## GPU Verification & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ec9a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üî• GPU SETUP VERIFICATION\n",
      "==================================================\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "cuDNN version: 90501\n",
      "\n",
      "üìä GPU Details:\n",
      "GPU count: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3070 Ti\n",
      "  Memory: 7.7 GB\n",
      "  Compute Capability: 8.6\n",
      "  Multiprocessors: 48\n",
      "\n",
      "üíæ Current GPU Memory:\n",
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n",
      "\n",
      "üñ•Ô∏è  CPU Info:\n",
      "CPU cores: 16\n",
      "RAM: 31.2 GB\n"
     ]
    }
   ],
   "source": [
    "import torch    #type: ignore\n",
    "import torch.nn as nn   #type: ignore\n",
    "from transformers import AutoTokenizer, AutoModel   #type: ignore\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Comprehensive GPU info\n",
    "def check_gpu_setup():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üî• GPU SETUP VERIFICATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # PyTorch and CUDA\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nüìä GPU Details:\")\n",
    "        print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"GPU {i}: {props.name}\")\n",
    "            print(f\"  Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "            print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "            print(f\"  Multiprocessors: {props.multi_processor_count}\")\n",
    "        \n",
    "        # Memory usage\n",
    "        print(f\"\\nüíæ Current GPU Memory:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå GPU not available - will use CPU\")\n",
    "    \n",
    "    # CPU info as backup\n",
    "    print(f\"\\nüñ•Ô∏è  CPU Info:\")\n",
    "    print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "    print(f\"RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "    \n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "gpu_available = check_gpu_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72f6af",
   "metadata": {},
   "source": [
    "## Optimal Batch Size Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424aabf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def find_optimal_batch_size(model_name=\"distilbert-base-uncased\", max_length=512):\n",
    "    \"\"\"\n",
    "    Find the optimal batch size for your GPU to maximize utilization\n",
    "    without running out of memory.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU available - recommended batch size: 8-16\")\n",
    "        return 16\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"Testing with {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Test different batch sizes\n",
    "    batch_sizes = [8, 16, 24, 32, 48, 64, 96, 128]\n",
    "    optimal_batch_size = 8\n",
    "    \n",
    "    print(\"\\nüß™ Testing batch sizes:\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Create dummy batch\n",
    "            dummy_texts = [\"This is a test sentence for batch size optimization.\"] * batch_size\n",
    "            inputs = tokenizer(\n",
    "                dummy_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Test forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # Check memory usage\n",
    "            memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "            memory_cached = torch.cuda.memory_reserved() / 1024**3\n",
    "            \n",
    "            print(f\"  Batch size {batch_size:3d}: ‚úÖ Memory: {memory_used:.2f}GB allocated, {memory_cached:.2f}GB cached\")\n",
    "            optimal_batch_size = batch_size\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"  Batch size {batch_size:3d}: ‚ùå Out of memory\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  Batch size {batch_size:3d}: ‚ùå Error: {e}\")\n",
    "                break\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nüéØ Recommended batch size: {optimal_batch_size}\")\n",
    "    print(f\"üí° For training, use batch size: {max(optimal_batch_size // 2, 8)} (with safety margin)\")\n",
    "    \n",
    "    return max(optimal_batch_size // 2, 8)\n",
    "\n",
    "# Test optimal batch size\n",
    "optimal_batch = find_optimal_batch_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01079efe",
   "metadata": {},
   "source": [
    "## GPU Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beebe93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# GPU-optimized configuration\n",
    "GPU_CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"distilbert-base-uncased\",  # Perfect for RTX 3070 Ti\n",
    "    \"max_length\": 512,\n",
    "    \"num_labels\": 3,\n",
    "    \n",
    "    # Training settings optimized for your GPU\n",
    "    \"batch_size\": optimal_batch,  # Dynamically determined\n",
    "    \"learning_rate\": 3e-5,        # Slightly higher for GPU training\n",
    "    \"num_epochs\": 5,              # More epochs with faster training\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # GPU-specific optimizations\n",
    "    \"fp16\": True,                 # Mixed precision for speed + memory\n",
    "    \"dataloader_pin_memory\": True,\n",
    "    \"dataloader_num_workers\": 4,  # Parallel data loading\n",
    "    \n",
    "    # Monitoring (faster with GPU)\n",
    "    \"logging_steps\": 50,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_steps\": 500,\n",
    "    \n",
    "    # Safety settings\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"early_stopping_patience\": 3\n",
    "}\n",
    "\n",
    "print(\"üöÄ GPU Training Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in GPU_CONFIG.items():\n",
    "    print(f\"{key:25}: {value}\")\n",
    "\n",
    "# Save config for later use\n",
    "import json\n",
    "with open(\"../config/gpu_config.json\", \"w\") as f:\n",
    "    json.dump(GPU_CONFIG, f, indent=2)\n",
    "    \n",
    "print(\"\\nüíæ Configuration saved to config/gpu_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f78977",
   "metadata": {},
   "source": [
    "## Performance Expectations with Your Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24746cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def estimate_training_time(num_samples, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Estimate training time based on your RTX 3070 Ti performance.\n",
    "    \"\"\"\n",
    "    # Rough estimates based on RTX 3070 Ti benchmarks\n",
    "    seconds_per_batch_distilbert = 0.15  # DistilBERT on RTX 3070 Ti\n",
    "    seconds_per_batch_bert = 0.25        # BERT-base on RTX 3070 Ti\n",
    "    \n",
    "    batches_per_epoch = num_samples // batch_size\n",
    "    total_batches = batches_per_epoch * num_epochs\n",
    "    \n",
    "    # Estimates for different models\n",
    "    distilbert_time = total_batches * seconds_per_batch_distilbert\n",
    "    bert_time = total_batches * seconds_per_batch_bert\n",
    "    \n",
    "    print(f\"üìä Training Time Estimates for {num_samples:,} samples:\")\n",
    "    print(f\"{'Model':<15} {'Time':<12} {'Speed':<15} {'Recommended':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'DistilBERT':<15} {distilbert_time/60:.1f} min{'s':<6} {'‚ö° Very Fast':<15} {'‚úÖ Ideal':<12}\")\n",
    "    print(f\"{'BERT-base':<15} {bert_time/60:.1f} min{'s':<6} {'üöÄ Fast':<15} {'‚úÖ Good':<12}\")\n",
    "    print(f\"{'RoBERTa':<15} {bert_time*1.2/60:.1f} min{'s':<6} {'üêå Slower':<15} {'‚ö†Ô∏è  Heavy':<12}\")\n",
    "    \n",
    "    return distilbert_time, bert_time\n",
    "\n",
    "# Estimates for different dataset sizes\n",
    "print(\"üéØ Performance Projections:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dataset_sizes = [5000, 10000, 25000, 50000]\n",
    "for size in dataset_sizes:\n",
    "    print(f\"\\nüìà Dataset size: {size:,} samples\")\n",
    "    estimate_training_time(size, optimal_batch, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198642f",
   "metadata": {},
   "source": [
    "## Memory Management for Long Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f62e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def setup_gpu_optimization():\n",
    "    \"\"\"\n",
    "    Configure PyTorch for optimal GPU performance.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Enable optimizations\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        \n",
    "        # Memory management\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"üîß GPU Optimizations Enabled:\")\n",
    "        print(f\"  ‚úÖ cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "        print(f\"  ‚úÖ cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "        print(f\"  ‚úÖ Memory cache cleared\")\n",
    "        \n",
    "        # Set memory allocation strategy\n",
    "        # This helps with memory fragmentation\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        print(f\"  ‚úÖ Memory allocation strategy optimized\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No GPU available - skipping GPU optimizations\")\n",
    "\n",
    "def monitor_gpu_usage():\n",
    "    \"\"\"\n",
    "    Monitor GPU memory usage during training.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(f\"üîç GPU Memory: {allocated:.2f}GB allocated, {cached:.2f}GB cached, {total:.1f}GB total\")\n",
    "        print(f\"üìä Usage: {(cached/total)*100:.1f}% of total GPU memory\")\n",
    "        \n",
    "        if cached > total * 0.9:\n",
    "            print(\"‚ö†Ô∏è  Warning: High memory usage - consider reducing batch size\")\n",
    "        elif cached < total * 0.5:\n",
    "            print(\"üí° Tip: You could potentially increase batch size for better performance\")\n",
    "    \n",
    "# Apply optimizations\n",
    "setup_gpu_optimization()\n",
    "monitor_gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c66c7",
   "metadata": {},
   "source": [
    "## FP16 Mixed Precision Setup & Testing üöÄ\n",
    "\n",
    "**Mixed precision training uses both FP16 and FP32 for optimal speed and stability.**\n",
    "\n",
    "### Benefits for RTX 3070 Ti:\n",
    "- ‚ö° **30-40% faster training**\n",
    "- üíæ **50% less memory usage** \n",
    "- üî• **Tensor Core acceleration**\n",
    "- üìà **Larger batch sizes possible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172602a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def test_fp16_support():\n",
    "    \"\"\"\n",
    "    Test if your GPU supports FP16 and measure performance difference.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ùå No GPU available - FP16 requires CUDA\")\n",
    "        return False\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Check Tensor Core support\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    has_tensor_cores = props.major >= 7  # Volta (V100) and newer\n",
    "    \n",
    "    print(\"üîç FP16 Capability Check:\")\n",
    "    print(f\"  GPU: {props.name}\")\n",
    "    print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"  Tensor Cores: {'‚úÖ Yes' if has_tensor_cores else '‚ùå No'}\")\n",
    "    \n",
    "    if not has_tensor_cores:\n",
    "        print(\"‚ö†Ô∏è  Warning: GPU doesn't have Tensor Cores, FP16 benefits will be limited\")\n",
    "    \n",
    "    # Test FP16 operations\n",
    "    try:\n",
    "        # Create test tensors\n",
    "        x = torch.randn(1000, 1000, device=device, dtype=torch.float16)\n",
    "        y = torch.randn(1000, 1000, device=device, dtype=torch.float16)\n",
    "        \n",
    "        # Test basic operations\n",
    "        z = torch.matmul(x, y)\n",
    "        \n",
    "        # Test autocast\n",
    "        from torch.cuda.amp import autocast #type: ignore\n",
    "        with autocast():\n",
    "            z_auto = torch.matmul(x.float(), y.float())\n",
    "        \n",
    "        print(\"  Basic FP16 operations: ‚úÖ Working\")\n",
    "        print(\"  Automatic Mixed Precision: ‚úÖ Working\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  FP16 test failed: ‚ùå {e}\")\n",
    "        return False\n",
    "\n",
    "def benchmark_fp16_vs_fp32():\n",
    "    \"\"\"\n",
    "    Benchmark training speed difference between FP16 and FP32.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU available for benchmarking\")\n",
    "        return\n",
    "    \n",
    "    from torch.cuda.amp import GradScaler, autocast #type: ignore\n",
    "    import time\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Create a simple model for testing\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 3)  # 3 classes for sentiment\n",
    "    ).to(device)\n",
    "    \n",
    "    # Test data\n",
    "    batch_size = 32\n",
    "    x = torch.randn(batch_size, 512, device=device)\n",
    "    y = torch.randint(0, 3, (batch_size,), device=device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"\\nüèÅ FP16 vs FP32 Benchmark:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # FP32 Benchmark\n",
    "    model.train()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    fp32_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"FP32 Training: {fp32_time:.3f} seconds (100 steps)\")\n",
    "    \n",
    "    # FP16 Benchmark\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    fp16_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"FP16 Training: {fp16_time:.3f} seconds (100 steps)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    speedup = fp32_time / fp16_time\n",
    "    memory_saved = (1 - fp16_time/fp32_time) * 100\n",
    "    \n",
    "    print(f\"\\nüöÄ Performance Results:\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x faster with FP16\")\n",
    "    print(f\"  Time reduction: {((fp32_time - fp16_time)/fp32_time)*100:.1f}%\")\n",
    "    \n",
    "    if speedup > 1.2:\n",
    "        print(\"  ‚úÖ Significant speedup - FP16 recommended!\")\n",
    "    elif speedup > 1.05:\n",
    "        print(\"  ‚ö†Ô∏è  Modest speedup - FP16 still beneficial\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Limited speedup - check GPU compatibility\")\n",
    "\n",
    "# Run FP16 tests\n",
    "print(\"Testing FP16 support on your RTX 3070 Ti...\")\n",
    "fp16_supported = test_fp16_support()\n",
    "\n",
    "if fp16_supported:\n",
    "    print(\"\\nüß™ Running performance benchmark...\")\n",
    "    benchmark_fp16_vs_fp32()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  FP16 not fully supported - falling back to FP32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b8e9f",
   "metadata": {},
   "source": [
    "### How to Enable FP16 in Your Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a9d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Method 1: Manual FP16 with GradScaler (Recommended)\n",
    "def create_fp16_training_example():\n",
    "    \"\"\"\n",
    "    Shows how to implement FP16 training for your FinSent model.\n",
    "    \"\"\"\n",
    "    from torch.cuda.amp import GradScaler, autocast     #type: ignore\n",
    "    from transformers import AutoModel, AutoTokenizer   #type: ignore\n",
    "    \n",
    "    print(\"üöÄ FP16 Training Implementation Example:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This is the pattern you'll use in your actual training code\n",
    "    example_code = '''\n",
    "# Import required components\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the scaler for FP16\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Your training loop\n",
    "def train_with_fp16(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # üî• FP16 Forward Pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                           attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        # üî• FP16 Backward Pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # üî• Optimizer step with scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "    '''\n",
    "    \n",
    "    print(\"üìù Copy this pattern for your training:\")\n",
    "    print(example_code)\n",
    "    \n",
    "    return example_code\n",
    "\n",
    "# Method 2: Hugging Face Transformers with FP16\n",
    "def create_huggingface_fp16_example():\n",
    "    \"\"\"\n",
    "    Shows how to enable FP16 with Hugging Face Trainer.\n",
    "    \"\"\"\n",
    "    \n",
    "    example_code = '''\n",
    "# Using Hugging Face Trainer with FP16 (Even Easier!)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,    # Larger batch with FP16\n",
    "    per_device_eval_batch_size=64,     # Even larger for eval\n",
    "    learning_rate=3e-5,\n",
    "    \n",
    "    # üî• Enable FP16 - Just one line!\n",
    "    fp16=True,                         # Enable mixed precision\n",
    "    fp16_opt_level=\"O1\",              # Conservative optimization\n",
    "    \n",
    "    # Optional FP16 settings\n",
    "    dataloader_pin_memory=True,        # Faster data transfer\n",
    "    dataloader_num_workers=4,          # Parallel data loading\n",
    "    \n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create trainer with FP16 enabled\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train with automatic FP16!\n",
    "trainer.train()\n",
    "    '''\n",
    "    \n",
    "    print(\"\\nü§ñ Hugging Face Trainer with FP16:\")\n",
    "    print(example_code)\n",
    "    \n",
    "    return example_code\n",
    "\n",
    "# Method 3: Safety Wrapper for FP16\n",
    "def create_fp16_safety_wrapper():\n",
    "    \"\"\"\n",
    "    Safe FP16 implementation with fallback to FP32.\n",
    "    \"\"\"\n",
    "    \n",
    "    safety_code = '''\n",
    "# Safe FP16 Training with Automatic Fallback\n",
    "class SafeFP16Trainer:\n",
    "    def __init__(self, model, use_fp16=True):\n",
    "        self.model = model\n",
    "        self.use_fp16 = use_fp16 and torch.cuda.is_available()\n",
    "        self.scaler = GradScaler() if self.use_fp16 else None\n",
    "        \n",
    "        print(f\"üîß Training mode: {'FP16' if self.use_fp16 else 'FP32'}\")\n",
    "    \n",
    "    def training_step(self, batch, optimizer, criterion):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            if self.use_fp16:\n",
    "                # FP16 training\n",
    "                with autocast():\n",
    "                    outputs = self.model(**batch)\n",
    "                    loss = criterion(outputs.logits, batch['labels'])\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # Check for inf/nan gradients\n",
    "                self.scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                self.scaler.step(optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "            else:\n",
    "                # FP32 training (fallback)\n",
    "                outputs = self.model(**batch)\n",
    "                loss = criterion(outputs.logits, batch['labels'])\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            return loss.item()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"nan\" in str(e).lower() or \"inf\" in str(e).lower():\n",
    "                print(\"‚ö†Ô∏è  FP16 instability detected, falling back to FP32\")\n",
    "                self.use_fp16 = False\n",
    "                self.scaler = None\n",
    "                return self.training_step(batch, optimizer, criterion)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# Usage:\n",
    "# trainer = SafeFP16Trainer(model, use_fp16=True)\n",
    "# loss = trainer.training_step(batch, optimizer, criterion)\n",
    "    '''\n",
    "    \n",
    "    print(\"\\nüõ°Ô∏è  Safe FP16 Implementation:\")\n",
    "    print(safety_code)\n",
    "    \n",
    "    return safety_code\n",
    "\n",
    "# Show all implementation methods\n",
    "print(\"Here are 3 ways to enable FP16 in your project:\\n\")\n",
    "\n",
    "method1 = create_fp16_training_example()\n",
    "method2 = create_huggingface_fp16_example()  \n",
    "method3 = create_fp16_safety_wrapper()\n",
    "\n",
    "print(f\"\\n‚úÖ Choose the method that fits your training approach!\")\n",
    "print(f\"   ‚Ä¢ Method 1: Manual control (most flexible)\")\n",
    "print(f\"   ‚Ä¢ Method 2: Hugging Face Trainer (easiest)\")  \n",
    "print(f\"   ‚Ä¢ Method 3: Safety wrapper (most robust)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9743b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Update your GPU config with FP16 settings\n",
    "if fp16_supported:\n",
    "    # Update the config with FP16 optimizations\n",
    "    GPU_CONFIG_FP16 = GPU_CONFIG.copy()\n",
    "    GPU_CONFIG_FP16.update({\n",
    "        # FP16 specific settings\n",
    "        \"fp16\": True,\n",
    "        \"fp16_opt_level\": \"O1\",              # Conservative mixed precision\n",
    "        \"fp16_full_eval\": False,             # Keep evaluation in FP32 for stability\n",
    "        \n",
    "        # Increase batch size with memory savings\n",
    "        \"batch_size\": min(optimal_batch + 16, 64),  # Larger batch with FP16\n",
    "        \"gradient_accumulation_steps\": 1,     # Less needed with larger batches\n",
    "        \n",
    "        # Optimize data loading for FP16\n",
    "        \"dataloader_pin_memory\": True,\n",
    "        \"dataloader_num_workers\": 4,\n",
    "        \n",
    "        # Safety settings for FP16\n",
    "        \"max_grad_norm\": 1.0,                # Gradient clipping\n",
    "        \"loss_scale\": 0,                     # Dynamic loss scaling\n",
    "        \"loss_scale_window\": 1000,           # Loss scaling window\n",
    "    })\n",
    "    \n",
    "    print(\"üî• Updated GPU Configuration with FP16:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Show the differences\n",
    "    for key, value in GPU_CONFIG_FP16.items():\n",
    "        if key not in GPU_CONFIG or GPU_CONFIG[key] != value:\n",
    "            print(f\"{key:25}: {value} {'üÜï' if key not in GPU_CONFIG else '‚¨ÜÔ∏è'}\")\n",
    "        else:\n",
    "            print(f\"{key:25}: {value}\")\n",
    "    \n",
    "    # Save updated config\n",
    "    with open(\"../config/gpu_config_fp16.json\", \"w\") as f:\n",
    "        json.dump(GPU_CONFIG_FP16, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ FP16-optimized config saved to config/gpu_config_fp16.json\")\n",
    "    print(f\"üìà Expected improvements:\")\n",
    "    print(f\"   ‚Ä¢ Training speed: 30-40% faster\")\n",
    "    print(f\"   ‚Ä¢ Memory usage: ~50% reduction\") \n",
    "    print(f\"   ‚Ä¢ Batch size: {GPU_CONFIG['batch_size']} ‚Üí {GPU_CONFIG_FP16['batch_size']}\")\n",
    "    print(f\"   ‚Ä¢ Model size: Can fit larger models in 8GB VRAM\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  FP16 not fully supported - keeping FP32 configuration\")\n",
    "    print(\"üí° You can still use the project effectively with FP32!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41036c4",
   "metadata": {},
   "source": [
    "## Ready for GPU-Accelerated Training! üöÄ\n",
    "//TODO: Delete Later\n",
    "### Your Optimal Setup:\n",
    "- **GPU**: RTX 3070 Ti (8GB VRAM) ‚úÖ\n",
    "- **Batch Size**: Dynamically optimized\n",
    "- **Model**: DistilBERT (perfect balance of speed/accuracy)\n",
    "- **Training Time**: ~5-15 minutes for most datasets\n",
    "- **Memory**: Optimized allocation strategy\n",
    "\n",
    "### Next Steps:\n",
    "1. **Complete data exploration** with confidence you have great hardware\n",
    "2. **Use larger datasets** - your GPU can handle 25k-50k samples easily\n",
    "3. **Experiment freely** - fast training means quick iteration\n",
    "4. **Try advanced models** - BERT, RoBERTa are viable options\n",
    "\n",
    "### Pro Tips for Your Setup:\n",
    "- **Mixed precision (fp16)** will give you ~30% speedup\n",
    "- **Larger batch sizes** = better GPU utilization\n",
    "- **More epochs** = better convergence (training is fast!)\n",
    "- **Save checkpoints** frequently for experimentation\n",
    "\n",
    "**You're ready to build something impressive! üî•**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
