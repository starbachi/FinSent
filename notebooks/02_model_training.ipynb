{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ad9568",
   "metadata": {},
   "source": [
    "# 02. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bff30",
   "metadata": {},
   "source": [
    "**Target Metric: F1 Score**\n",
    "\n",
    "**Reason: Imbalanced Label Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8d9c1",
   "metadata": {},
   "source": [
    "## 0. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1751aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 17:00:55.906130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755014455.917668   16260 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755014455.921105   16260 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755014455.931308   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755014455.931319   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755014455.931320   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755014455.931321   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-12 17:00:55.934463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import platform, subprocess, psutil, torch, transformers, os, logging, warnings, json, yaml\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments,              #type: ignore\n",
    "    Trainer,                        #type: ignore\n",
    "    EarlyStoppingCallback,          #type: ignore\n",
    "    get_linear_schedule_with_warmup #type: ignore\n",
    ")\n",
    "\n",
    "# ----------------------------------- SETUP ---------------------------------- #\n",
    "\n",
    "# Load configuration\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# matplotlib inline setup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60291bf7",
   "metadata": {},
   "source": [
    "## 1. System Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c4efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux 6.8.0-65-generic\n",
      "Python version: 3.10.12\n",
      "Architecture: x86_64\n",
      "Total RAM: 31.2 GB\n",
      "Available RAM: 21.6 GB\n",
      "RAM Usage: 30.8%\n",
      "Total Disk: 419.7 GB\n",
      "Free Disk: 338.8 GB\n",
      "GPU CHECK\n",
      "✓ NVIDIA GPU detected\n",
      "GPU: |   0  NVIDIA GeForce RTX 3070 Ti     On  |   00000000:07:00.0  On |                  N/A |\n",
      "PyTorch version: 2.7.1+cu126\n",
      "✓ CUDA available: 12.6\n",
      "✓ GPU count: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 3070 Ti (7.7 GB)\n",
      "TensorFlow version: 2.19.0\n",
      "✓ TensorFlow GPU support: 1 GPU(s)\n",
      "  GPU 0: /physical_device:GPU:0\n",
      "Transformers version: 4.54.1\n",
      "\n",
      "RECOMMENDATIONS\n",
      "TIP: GPU has limited memory - use smaller batch sizes and gradient accumulation\n",
      "\n",
      "System constraints check complete\n",
      "\n",
      "GPU ACTIVATION\n",
      "GPU activated: NVIDIA GeForce RTX 3070 Ti\n",
      "  Device: cuda\n",
      "  Memory allocated: 0.00 GB\n",
      "  Memory reserved: 0.00 GB\n",
      "  GPU cache cleared\n",
      "Training device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "def check_system_constraints():\n",
    "    \"\"\"Check system resources and constraints for model training\"\"\"\n",
    "    \n",
    "    # Basic system info\n",
    "    print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Python version: {platform.python_version()}\")\n",
    "    print(f\"Architecture: {platform.machine()}\")\n",
    "    \n",
    "    # Check available memory\n",
    "    try:\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "        print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "        print(f\"RAM Usage: {memory.percent}%\")\n",
    "        \n",
    "        # Disk space\n",
    "        disk = psutil.disk_usage('/')\n",
    "        print(f\"Total Disk: {disk.total / (1024**3):.1f} GB\")\n",
    "        print(f\"Free Disk: {disk.free / (1024**3):.1f} GB\")\n",
    "    except ImportError:\n",
    "        print(\"psutil not installed - install with: pip install psutil\")\n",
    "    \n",
    "    # Check CUDA/GPU availability\n",
    "    print(\"GPU CHECK\")\n",
    "    \n",
    "    # Check NVIDIA GPU\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"✓ NVIDIA GPU detected\")\n",
    "            # Extract basic GPU info\n",
    "            lines = result.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'Tesla' in line or 'GeForce' in line or 'Quadro' in line or 'RTX' in line or 'GTX' in line:\n",
    "                    print(f\"GPU: {line.strip()}\")\n",
    "        else:\n",
    "            print(\"✗ NVIDIA GPU not detected or nvidia-smi not available\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ nvidia-smi not found\")\n",
    "    \n",
    "    # Check PyTorch GPU support\n",
    "    try:\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"✓ CUDA available: {torch.version.cuda}\")   #type: ignore\n",
    "            print(f\"✓ GPU count: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")\n",
    "        else:\n",
    "            print(\"✗ CUDA not available in PyTorch\")\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not installed\")\n",
    "    \n",
    "    # Check TensorFlow GPU support\n",
    "    try:\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(f\"✓ TensorFlow GPU support: {len(gpus)} GPU(s)\")\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"  GPU {i}: {gpu.name}\")\n",
    "        else:\n",
    "            print(\"✗ No GPU support in TensorFlow\")\n",
    "    except ImportError:\n",
    "        print(\"TensorFlow not installed\")\n",
    "    \n",
    "    # Check transformers library\n",
    "    try:\n",
    "        print(f\"Transformers version: {transformers.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"Transformers library not installed\")\n",
    "        \n",
    "    print(\"\\nRECOMMENDATIONS\")\n",
    "    \n",
    "    # Memory recommendations\n",
    "    try:\n",
    "        if memory.available / (1024**3) < 8:    #type: ignore\n",
    "            print(\"WARNING: Less than 8GB RAM available\")\n",
    "            print(\"   Consider closing other applications or using smaller batch sizes\")\n",
    "        \n",
    "        if disk.free / (1024**3) < 10:          #type: ignore\n",
    "            print(\"WARNING: Less than 10GB disk space available\")\n",
    "            print(\"   Model checkpoints and datasets may require significant space\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # GPU recommendations\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"TIP: Training will use CPU only - consider using Google Colab or cloud GPU\")\n",
    "        elif torch.cuda.device_count() == 1:\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            if props.total_memory < 8 * 1024**3:  # Less than 8GB VRAM\n",
    "                print(\"TIP: GPU has limited memory - use smaller batch sizes and gradient accumulation\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\nSystem constraints check complete\")\n",
    "    \n",
    "    # Return device information for use in training\n",
    "    return {\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'memory_gb': memory.available / (1024**3) if 'memory' in locals() else None #type: ignore\n",
    "    }\n",
    "\n",
    "# Run the check and get system info\n",
    "system_info = check_system_constraints()\n",
    "\n",
    "# ------------------------------ GPU ACTIVATION ------------------------------ #\n",
    "print(\"\\nGPU ACTIVATION\")\n",
    "\n",
    "# Set up device for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU activated: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Set memory allocation strategy (optional)\n",
    "    torch.cuda.empty_cache()  # Clear cache\n",
    "    print(\"  GPU cache cleared\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for training\")\n",
    "    print(f\"  Device: {device}\")\n",
    "\n",
    "print(f\"Training device set to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c914ee",
   "metadata": {},
   "source": [
    "## 2. Load Training, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b016bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3232, Val set size: 693, Test set size: 693\n",
      "\n",
      "Train \n",
      "                                                 text  label\n",
      "0  The contract covers new energy-efficient AC dr...      1\n",
      "1  The cranes would be installed onboard two frei...      1\n",
      "2  Inca Contract Manufacturing will carry out the...      1\n",
      "3  Finnish metal components supplier Component AY...      2\n",
      "4  ` Low energy consumption and flexible loading ...      1\n",
      "\n",
      "Validation \n",
      "                                                 text  label\n",
      "0  Concord would focus on the development, manufa...      1\n",
      "1  Why put up costly cell phone towers in thinly ...      1\n",
      "2  Forum needs a clear signal of commitment from ...      1\n",
      "3  TomTom has given assurances that it will conti...      2\n",
      "4  The company said it observed a current stabili...      2\n",
      "\n",
      "Test \n",
      "                                                 text  label\n",
      "0  The Swedish player became majority owner of Ce...      1\n",
      "1  Ruukki's order book at the end of 2010 was 30%...      2\n",
      "2  Operating profit improved by 39.9% to EUR 18.0...      2\n",
      "3  The contractor of the shopping center, China S...      1\n",
      "4  Chapman, the Stockholm-headquartered private e...      1\n"
     ]
    }
   ],
   "source": [
    "train, val, test = config['data']['train_data'], config['data']['val_data'], config['data']['test_data']\n",
    "\n",
    "train, val, test = pd.read_csv(train), pd.read_csv(val), pd.read_csv(test)\n",
    "\n",
    "print(f\"Train set size: {len(train)}, Val set size: {len(val)}, Test set size: {len(test)}\")\n",
    "print(\"\")\n",
    "print(f\"Train \\n {train.head()}\")\n",
    "print(\"\")\n",
    "print(f\"Validation \\n {val.head()}\")\n",
    "print(\"\")\n",
    "print(f\"Test \\n {test.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67876129",
   "metadata": {},
   "source": [
    "## 3. Load Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df5a14",
   "metadata": {},
   "source": [
    "### 3.1 Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b3986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ LOAD METADATA ------------------------------ #\n",
    "metadata = config['data']['metadata']\n",
    "with open(metadata, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "print(\"Metadata loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b9774",
   "metadata": {},
   "source": [
    "### 3.2 Display Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8425539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "   Total samples: 4,618\n",
      "   Number of classes: 3\n",
      "   Class names: ['negative', 'positive', 'neutral']\n",
      "   Processing date: 2025-08-09T17:27:36.071920\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ DISPLAY DATASET INFORMATION ----------------------- #\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"   Total samples: {metadata['dataset_info']['total_samples']:,}\")\n",
    "print(f\"   Number of classes: {metadata['dataset_info']['num_classes']}\")\n",
    "print(f\"   Class names: {metadata['dataset_info']['class_names']}\")\n",
    "print(f\"   Processing date: {metadata['dataset_info']['processing_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09874811",
   "metadata": {},
   "source": [
    "### 3.3 Display Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c484d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Splits:\n",
      "   Train: 3,232 samples (70.0%)\n",
      "   Val: 693 samples (15.0%)\n",
      "   Test: 693 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- DISPLAY DATA SPLITS --------------------------- #\n",
    "print(f\"\\nData Splits:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if split in metadata['splits']:\n",
    "        size = metadata['splits'][split]['size']\n",
    "        pct = metadata['splits'][split]['percentage']\n",
    "        print(f\"   {split.capitalize()}: {size:,} samples ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9e159",
   "metadata": {},
   "source": [
    "### 3.4 Display Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9af72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution:\n",
      "   Train:\n",
      "      neutral: 1890\n",
      "      positive: 932\n",
      "      negative: 410\n",
      "   Val:\n",
      "      neutral: 405\n",
      "      positive: 200\n",
      "      negative: 88\n",
      "   Test:\n",
      "      neutral: 405\n",
      "      positive: 200\n",
      "      negative: 88\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ DISPLAY LABEL DISTRIBUTION ------------------------ #\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if split in metadata['label_distribution']:\n",
    "        print(f\"   {split.capitalize()}:\")\n",
    "        for label, count in metadata['label_distribution'][split].items():\n",
    "            print(f\"      {label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eec434",
   "metadata": {},
   "source": [
    "### 3.5 Display Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7c0c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Configuration:\n",
      "   Tokenizer: ProsusAI/finbert\n",
      "   Max length: 512\n",
      "   Vocab size: 30,522\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ DISPLAY MODEL CONFIGURATION ----------------------- #\n",
    "if 'model_info' in metadata:\n",
    "    print(f\"\\nModel Configuration:\")\n",
    "    print(f\"   Tokenizer: {metadata['model_info']['tokenizer']}\")\n",
    "    print(f\"   Max length: {metadata['model_info']['max_length']}\")\n",
    "    print(f\"   Vocab size: {metadata['model_info']['vocab_size']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a642b",
   "metadata": {},
   "source": [
    "### 3.6 Display Text Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48163b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Statistics:\n",
      "   Avg characters: 129.7\n",
      "   Avg words: 21.1\n",
      "   Word range: 7-52 words\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- DISPLAY TEXT STATISTICS ------------------------- #\n",
    "if 'text_statistics' in metadata:\n",
    "    print(f\"\\nText Statistics:\")\n",
    "    print(f\"   Avg characters: {metadata['text_statistics']['avg_char_length']:.1f}\")\n",
    "    print(f\"   Avg words: {metadata['text_statistics']['avg_word_length']:.1f}\")\n",
    "    print(f\"   Word range: {metadata['text_statistics']['min_word_length']}-{metadata['text_statistics']['max_word_length']} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444e019",
   "metadata": {},
   "source": [
    "### 3.7 Display Validation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674ec6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Validation Status: PASS\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- DISPLAY VALIDATION STATUS ------------------------ #\n",
    "if 'validation_results' in metadata:\n",
    "    status = metadata['validation_results']['overall_status']\n",
    "    print(f\"\\nData Validation Status: {status}\")\n",
    "    if status != 'PASS':\n",
    "        print(\"   Some validation issues detected - check preprocessing notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698386a0",
   "metadata": {},
   "source": [
    "### 3.8 Extract Important Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45082d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Parameters:\n",
      "   Overall label distribution: {'neutral': 2700, 'positive': 1332, 'negative': 586}\n",
      "   Class weights: {1: 0.5701234567901234, 2: 1.1556556556556556, 0: 2.626848691695108}\n",
      "   Number of classes: 3\n",
      "   Class names: ['negative', 'positive', 'neutral']\n",
      "   Max sequence length: 512\n",
      "   Tokenizer: ProsusAI/finbert\n",
      "\n",
      "Metadata analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# ------------------- EXTRACT IMPORTANT TRAINING PARAMETERS ------------------ #\n",
    "print(f\"\\nTraining Parameters:\")\n",
    "\n",
    "# Check the structure of label_distribution to find class weights\n",
    "if 'class_weights' in metadata['label_distribution']:\n",
    "    # If class weights are stored separately\n",
    "    class_weights_raw = metadata['label_distribution']['class_weights']\n",
    "    class_weights = {int(k): float(v) for k, v in class_weights_raw.items()}\n",
    "elif 'overall' in metadata['label_distribution']:\n",
    "    # If we need to calculate weights from overall distribution\n",
    "    overall_dist = metadata['label_distribution']['overall']\n",
    "    print(f\"   Overall label distribution: {overall_dist}\")\n",
    "    \n",
    "    # Calculate class weights if they're not pre-calculated\n",
    "    # This is a simple inverse frequency weighting\n",
    "    total_samples = sum(overall_dist.values())\n",
    "    num_classes = len(overall_dist)\n",
    "    class_weights = {}\n",
    "    \n",
    "    # Map label names to IDs (assuming order: negative=0, neutral=1, positive=2)\n",
    "    label_to_id = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    \n",
    "    for label, count in overall_dist.items():\n",
    "        class_id = label_to_id.get(label, len(label_to_id))\n",
    "        weight = total_samples / (num_classes * count)\n",
    "        class_weights[class_id] = weight\n",
    "else:\n",
    "    print(\"   Class weights not found in metadata, using equal weights\")\n",
    "    num_classes = metadata['dataset_info']['num_classes']\n",
    "    class_weights = {i: 1.0 for i in range(num_classes)}\n",
    "\n",
    "print(f\"   Class weights: {class_weights}\")\n",
    "\n",
    "# Also extract other useful parameters\n",
    "num_classes = metadata['dataset_info']['num_classes']\n",
    "class_names = metadata['dataset_info']['class_names']\n",
    "max_length = metadata['model_info']['max_length'] if 'model_info' in metadata else 512\n",
    "tokenizer_name = metadata['model_info']['tokenizer'] if 'model_info' in metadata else 'ProsusAI/finbert'\n",
    "\n",
    "print(f\"   Number of classes: {num_classes}\")\n",
    "print(f\"   Class names: {class_names}\")\n",
    "print(f\"   Max sequence length: {max_length}\")\n",
    "print(f\"   Tokenizer: {tokenizer_name}\")\n",
    "\n",
    "print(f\"\\nMetadata analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7d290",
   "metadata": {},
   "source": [
    "## 4. Load Label Encoder Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320c1d1",
   "metadata": {},
   "source": [
    "## 5. Load FinBERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a727324",
   "metadata": {},
   "source": [
    "## 6. Assign Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5005e9",
   "metadata": {},
   "source": [
    "## 7. Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfac275",
   "metadata": {},
   "source": [
    "Load the pre-trained \"ProsusAI/finbert\" model with a classification head\n",
    "\n",
    "Decide on fine-tuning strategy (freeze early layers vs. full fine-tuning)\n",
    "\n",
    "Consider adding dropout layers for regularization\n",
    "\n",
    "Set up the model to output logits for your 3 sentiment classes\n",
    "\n",
    "Move the model to your available device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d52399",
   "metadata": {},
   "source": [
    "Batch size: Start with 16 or 32, adjust based on GPU memory\n",
    "\n",
    "Learning rate: Typical range 1e-5 to 5e-5 for BERT fine-tuning\n",
    "\n",
    "Number of epochs: Usually 3-5 epochs for BERT models\n",
    "\n",
    "Optimizer: AdamW with weight decay\n",
    "\n",
    "Scheduler: Linear warmup with decay\n",
    "\n",
    "Early stopping: Monitor validation F1-score, not just loss\n",
    "\n",
    "Use the class weights you calculated in preprocessing\n",
    "\n",
    "Consider weighted loss function (CrossEntropyLoss with weight parameter)\n",
    "\n",
    "Alternative: Focal Loss for hard example mining\n",
    "\n",
    "Monitor per-class metrics, not just overall accuracy\n",
    "\n",
    "For F1-score focus:\n",
    "\n",
    "\n",
    "Implement custom compute_metrics function for Trainer\n",
    "\n",
    "Calculate macro-averaged F1, weighted F1, and per-class F1\n",
    "\n",
    "Include precision and recall for each class\n",
    "\n",
    "Generate confusion matrices for detailed analysis\n",
    "\n",
    "Output directory for model checkpoints\n",
    "\n",
    "Evaluation strategy (steps vs. epochs)\n",
    "\n",
    "Save strategy aligned with evaluation\n",
    "\n",
    "Load best model at end based on F1-score\n",
    "\n",
    "Gradient accumulation if using small batch sizes\n",
    "\n",
    "Mixed precision training (fp16) if GPU supports it\n",
    "\n",
    "Your model and tokenizer\n",
    "\n",
    "Training and validation datasets\n",
    "\n",
    "Training arguments\n",
    "\n",
    "Custom compute_metrics function\n",
    "\n",
    "Early stopping callback\n",
    "\n",
    "Potentially custom loss function with class weights\n",
    "\n",
    "Checkpoint saving strategy\n",
    "\n",
    "Logging frequency for monitoring\n",
    "\n",
    "Validation frequency\n",
    "\n",
    "Memory management during training\n",
    "\n",
    "How to handle potential GPU memory issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
