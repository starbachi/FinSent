{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e259adce",
   "metadata": {},
   "source": [
    "# 02. Data Preprocessing and Feature Engineering\n",
    "\n",
    "**Objective**: Prepare the data for model training by implementing robust preprocessing pipelines.\n",
    "\n",
    "## ðŸ“‹ Tasks for you to complete:\n",
    "1. Text cleaning and normalization\n",
    "2. Tokenization strategy\n",
    "3. Handling class imbalance\n",
    "4. Train/validation/test split\n",
    "5. Feature engineering for financial text\n",
    "\n",
    "## ðŸŽ¯ Learning Goals:\n",
    "- Production-ready data preprocessing\n",
    "- Domain-specific feature engineering\n",
    "- Robust data pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import preprocessing libraries\n",
    "# Hint: transformers (tokenizer), sklearn (preprocessing), re, string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "# Add your imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4531a",
   "metadata": {},
   "source": [
    "## Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement text cleaning functions\n",
    "# 1. Remove HTML tags, special characters\n",
    "# 2. Handle financial symbols ($, %, etc.)\n",
    "# 3. Normalize whitespace\n",
    "# 4. Optional: lowercase conversion\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize financial text data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "sample_text = \"The company's Q3 revenue increased by 15% ($2.5M) compared to last year!!!\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c0465",
   "metadata": {},
   "source": [
    "## Tokenization and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up tokenizer for transformer models\n",
    "# 1. Choose appropriate tokenizer (DistilBERT, BERT, RoBERTa)\n",
    "# 2. Configure max_length, padding, truncation\n",
    "# 3. Handle special tokens\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your tokenizer setup here\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # or your choice\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_texts(texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize texts for transformer models.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to tokenize\n",
    "        max_length (int): Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized inputs\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9770e8",
   "metadata": {},
   "source": [
    "## Label Encoding and Class Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Handle sentiment labels\n",
    "# 1. Encode sentiment labels to numbers\n",
    "# 2. Check class distribution\n",
    "# 3. Implement class balancing strategy if needed\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def encode_labels(sentiments):\n",
    "    \"\"\"\n",
    "    Encode sentiment labels to numerical values.\n",
    "    \n",
    "    Args:\n",
    "        sentiments (list): List of sentiment labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (encoded_labels, label_encoder)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    \"\"\"\n",
    "    Calculate class weights for handling imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        labels (array): Encoded labels\n",
    "        \n",
    "    Returns:\n",
    "        dict: Class weights\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55bd88d",
   "metadata": {},
   "source": [
    "## Financial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract financial domain features\n",
    "# 1. Financial keywords presence\n",
    "# 2. Numerical patterns (percentages, currencies)\n",
    "# 3. Sentiment-bearing word counts\n",
    "# 4. Text complexity metrics\n",
    "\n",
    "def extract_financial_features(text):\n",
    "    \"\"\"\n",
    "    Extract financial domain-specific features.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        dict: Financial features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Financial keywords\n",
    "    financial_keywords = [\n",
    "        'revenue', 'profit', 'loss', 'earnings', 'growth', \n",
    "        'decline', 'increase', 'decrease', 'market', 'stock'\n",
    "    ]\n",
    "    \n",
    "    # Your feature extraction here\n",
    "    # Example:\n",
    "    # features['has_percentage'] = bool(re.search(r'\\d+%', text))\n",
    "    # features['has_currency'] = bool(re.search(r'\\$\\d+', text))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test your function\n",
    "sample_text = \"Revenue increased by 15% to $2.5M this quarter\"\n",
    "print(extract_financial_features(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c21e50",
   "metadata": {},
   "source": [
    "## Data Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement stratified data splitting\n",
    "# 1. Train/validation/test split (70/15/15 or 80/10/10)\n",
    "# 2. Ensure stratification by sentiment class\n",
    "# 3. Set random seeds for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(texts, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train/validation/test sets with stratification.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts\n",
    "        labels (array): Encoded labels\n",
    "        test_size (float): Test set proportion\n",
    "        val_size (float): Validation set proportion  \n",
    "        random_state (int): Random seed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee41ef",
   "metadata": {},
   "source": [
    "## Data Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create complete preprocessing pipeline\n",
    "# 1. Combine all preprocessing steps\n",
    "# 2. Save processed data\n",
    "# 3. Create data loaders for training\n",
    "\n",
    "class FinancialDataProcessor:\n",
    "    \"\"\"\n",
    "    Complete data preprocessing pipeline for financial sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.label_encoder = None\n",
    "        \n",
    "    def fit(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Fit the processor on training data.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Training texts\n",
    "            labels (list): Training labels\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def transform(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        Transform texts and labels.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Texts to transform\n",
    "            labels (list, optional): Labels to transform\n",
    "            \n",
    "        Returns:\n",
    "            dict: Processed data\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the processor state.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Save path\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8ce14",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement data validation checks\n",
    "# 1. Check for data leakage between splits\n",
    "# 2. Validate tokenization results\n",
    "# 3. Ensure label distribution consistency\n",
    "# 4. Check for edge cases\n",
    "\n",
    "def validate_preprocessing(train_data, val_data, test_data):\n",
    "    \"\"\"\n",
    "    Validate preprocessing results.\n",
    "    \n",
    "    Args:\n",
    "        train_data (dict): Training data\n",
    "        val_data (dict): Validation data\n",
    "        test_data (dict): Test data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Validation results\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    \n",
    "    # Your validation checks here\n",
    "    # Example checks:\n",
    "    # - No text overlap between splits\n",
    "    # - Token sequence lengths are within limits\n",
    "    # - Label distributions are reasonable\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2b27f",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633da922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save processed datasets\n",
    "# 1. Save train/val/test splits\n",
    "# 2. Save preprocessing artifacts (tokenizer, label encoder)\n",
    "# 3. Create data manifest/metadata file\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_processed_data(data_dict, save_dir=\"../data/processed\"):\n",
    "    \"\"\"\n",
    "    Save processed data and metadata.\n",
    "    \n",
    "    Args:\n",
    "        data_dict (dict): Processed data\n",
    "        save_dir (str): Save directory\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Your save implementation here\n",
    "    pass\n",
    "\n",
    "# Usage example:\n",
    "# save_processed_data({\n",
    "#     'train': train_data,\n",
    "#     'val': val_data,\n",
    "#     'test': test_data,\n",
    "#     'metadata': metadata\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b7c4a",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Implementation Hints:\n",
    "\n",
    "### Text Cleaning:\n",
    "```python\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Handle financial symbols\n",
    "    text = re.sub(r'\\$([0-9,]+)', r'MONEY_\\1', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "```\n",
    "\n",
    "### Tokenization:\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "encoded = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "```\n",
    "\n",
    "### Class Weights:\n",
    "```python\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
